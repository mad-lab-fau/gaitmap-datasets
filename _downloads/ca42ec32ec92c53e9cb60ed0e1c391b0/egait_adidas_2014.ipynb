{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# EgaitAdidas2014 - Healthy Participants with MoCap reference\n\nThis dataset contains data from healthy participants walking with different speed levels through a motion capture\nvolume.\nThe dataset can be used to benchmark the performance of spatial parameter estimation methods based on foot worn IMUs.\n\n## General Information\nThe EgaitAdidas2014 dataset contains data healthy participants walking through a vicon motion capture system with one\nIMU attached to each foot.\n\nFor many participants data for SHIMMER3 and SHIMMER2 is available. The SHIMMER3 data is sampled at 204.8 Hz and the\nSHIMMER2R data at 102.4 Hz.\nThis also allows for a comparison of the two sensors.\n\nFor both IMUs we unify the coordinate system on loading as shown below:\n\n.. figure:: /images/coordinate_systems/coordinate_transform_shimmer2R_lateral_eGait.svg\n    :alt: coordinate system definition\n    :figclass: align-center\n\n.. figure:: /images/coordinate_systems/coordinate_transform_shimmer3_lateral_eGait.svg\n    :alt: coordinate system definition\n    :figclass: align-center\n\n\nParticipants where instructed to walk with a specific stride length and velocity to create more variation in the\ndata.\nFor each trial only a couple strides were recorded within the motion capture system.\nThe IMU data contains the entire recording.\nThis additional data can contain just some additional strides or entire different movements depending on the trial.\nWe recommend inspecting the specific trial in case of issues.\n\nThe Vicon motion capture system was sampled at 200 Hz.\nThe IMUs and the mocap system are synchronized using a wireless trigger allowing for proper comparison of the calculated\ntrajectories.\n\nReference (expert labeled based on IMU data) stride borders are provided for all strides that are recorded by both\nsystems.\n\nIn the following we will show how to interact with the dataset and how to make sense of the reference information.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-danger\"><h4>Warning</h4><p>For this example to work, you need to have a global config set containing the path to the dataset.\n             Check the `README.md` for more information.</p></div>\n\nFirst we create a simple instance of the dataset class.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from gaitmap_datasets import EgaitAdidas2014\nfrom gaitmap_datasets.utils import convert_segmented_stride_list\n\ndataset = EgaitAdidas2014()\ndataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that we have 5 levels in the metadata.\n\n- participant\n- sensortype (shimmer2, shimmer3)\n- stride_length (low, medium, high)\n- stride_velocity (low, medium, high)\n- repetition (1, 2, 3)\n\nThe `stride_length` and `stride_velocity` are the instructions given to the participants.\nFor each combination of these two parameters, 3 repetitions were recorded.\n\nHowever, for many participants data for at least some trials are missing for various technical issues.\n\nFor now we are selecting the data for one participant.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "subset = dataset.get_subset(participant=\"008\")\nsubset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For this participant we will have a look at the \"normal\" stride length and velocity trial of the shimmer2r sensor.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "trial = subset.get_subset(stride_length=\"normal\", stride_velocity=\"normal\", sensor=\"shimmer2r\", repetition=\"1\")\ntrial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The IMU data is stored in the `data` attribute, which is a dictionary of pandas dataframes.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sensor = \"left_sensor\"\nimu_data = trial.data[sensor]\nimu_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The mocap data is stored in the `marker_position_` attribute, which is a dictionary of pandas dataframes, too.\nNote, that sometimes there are NaN values at the start and the end of the data.\nIn these regions the mocap system was recording, but none of the markers were in frame.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mocap_data = trial.marker_position_[sensor]\nmocap_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Both data sources have the time as index, so that we can easily plot them together.\nWe converted the time axis so that the start of the **Mocap** data is the global 0.\nThis means that the IMU data will have **negative** time values for the datapoints before the MoCap start.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n\nfig, (ax1, ax2, ax3) = plt.subplots(3, 1, sharex=True)\nimu_data.filter(like=\"gyr\").plot(ax=ax1, legend=False)\nimu_data.filter(like=\"acc\").plot(ax=ax2, legend=True)\nmocap_data[[\"heel_z\"]].plot(ax=ax3)\n\nax1.set_ylabel(\"Gyroscope [deg/s]\")\nax2.set_ylabel(\"Acc. [m/s^2]\")\nax3.set_ylabel(\"Pos. [m]\")\n\nfig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the strides that are within the mocap volume, manually annotated stride labels based on the IMU data are\navailable.\nThey are provided in samples relative to the start of the IMU data stream.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "segmented_strides = trial.segmented_stride_list_\nsegmented_strides[sensor]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To get the events relative to the mocap data (i.e. in mocap samples relative to the start of the mocap data you can\nuse the `convert_events` method.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "trial.convert_events(segmented_strides, from_time_axis=\"imu\", to_time_axis=\"mocap\")[sensor]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Similarly, you can convert the events to the same time axis as the data\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "trial.convert_events(segmented_strides, from_time_axis=\"imu\", to_time_axis=\"time\")[sensor]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In addition to the segmented strides, we also provide a reference event list calculated based on the mocap data.\nThis has the same start and end per stride as the segmented strides, but has columns for the initial contact/heel\nstrike (ic), final contact/toe off (tc) and mid-stance (min_vel).\nThis information is provided in samples relative to the start of the mocap data stream.\n(Compare to the converted segmented strides above).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mocap_events = trial.mocap_events_\nmocap_events[sensor]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Like the segmented stride list, we can convert them to the same time axis as the data or IMU samples.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "trial.convert_events(mocap_events, from_time_axis=\"mocap\", to_time_axis=\"time\")[sensor]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below we plot the time converted event list into the plot from above\nIn the mocap plot we also add the mocap derived gait events.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, sharex=True)\nimu_data.filter(like=\"gyr\").plot(ax=ax1, legend=False)\nimu_data.filter(like=\"acc\").plot(ax=ax2, legend=True)\nmocap_data[[\"heel_z\"]].plot(ax=ax3)\nfor ax in (ax1, ax2, ax3):\n    for i, s in trial.convert_events(segmented_strides, from_time_axis=\"imu\", to_time_axis=\"time\")[sensor].iterrows():\n        ax.axvspan(s[\"start\"], s[\"end\"], alpha=0.2, color=\"C1\")\n\n# We plot the events in ax3\nfor marker, event_name in zip([\"o\", \"s\", \"*\"], [\"tc\", \"ic\", \"min_vel\"]):\n    mocap_data[[\"heel_z\"]].iloc[mocap_events[sensor][event_name]].rename(columns={\"heel_z\": event_name}).plot(\n        ax=ax3, style=marker, label=event_name, markersize=3\n    )\n\nax1.set_ylabel(\"Gyroscope [deg/s]\")\nax2.set_ylabel(\"Acc. [m/s^2]\")\nax3.set_ylabel(\"Pos. [m]\")\n\nfig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see, in this example, three strides are properly detected by both systems.\nThese strides are defined based on the signal maximum in the `gyr_y` (i.e. `gyr_ml` axis).\n\nThis definition is good for segmentation.\nHowever, for calculation of gait parameters, the authors of the dataset defined strides from midstance (i.e. the\n`min_vel` point) to midstance of two consecutive strides.\nIn result, when looking at the parameters, there will be one stride less than the number of strides in the segmented\nstride list.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "trial.mocap_parameters_[sensor]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To better understand how this works, we can convert the mocap events from their segmented stride list form into a\nmin_vel-stride list.\nIn this form, the start and the end of each stride is defined by the `min_vel` event.\nIn addition, a new `pre_ic` event is added.\nThis marks the ic of the previous stride.\n\nOverall, one less stride exists in the min_vel stride list than in the segmented stride list.\nThe `s_id` of the new stride list is based on the `s_id` of the segmented stride that contains the `pre_ic` event.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mocap_min_vel_stride_list = convert_segmented_stride_list(mocap_events, target_stride_type=\"min_vel\")\nmocap_min_vel_stride_list[sensor]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Stride time is now calculated from the `pre_ic` to the `ic` event (compare `trial.mocap_parameters_[sensor]`).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "stride_time = mocap_min_vel_stride_list[sensor][\"ic\"] - mocap_min_vel_stride_list[sensor][\"pre_ic\"]\nstride_time / trial.mocap_sampling_rate_hz_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As comparison the pre-calculated stride time:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "trial.mocap_parameters_[sensor][\"stride_time\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Stride length is calculated as the displacement in the ground-plane between start and end (i.e. the two `min_vel`\nevents).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "starts = mocap_min_vel_stride_list[sensor][\"start\"]\nends = mocap_min_vel_stride_list[sensor][\"end\"]\nstride_length_heel = (\n    (\n        mocap_data[[\"heel_x\", \"heel_y\"]].iloc[ends].reset_index(drop=True)\n        - mocap_data[[\"heel_x\", \"heel_y\"]].iloc[starts].reset_index(drop=True)\n    )\n    .pow(2)\n    .sum(axis=1)\n    .pow(0.5)\n)\nstride_length_heel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As comparison the pre-calculated stride length:\nNote that this stride-length differs slightly from the one calculated above, as the authors of the dataset provided\nthe average stride length over all available markers.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "trial.mocap_parameters_[sensor][\"stride_length\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Usage as validation dataset\nTo compare the reference parameters with the parameters of a IMU based algorithm, you should use the segmented\nstride list as a starting point.\nFrom there you can calculate gait events (e.g. ic) within these strides to compare temporal parameters.\nIdeally store the events as a segmented stride list and then use the `convert_segmented_stride_list` function to\nbring them in the same format used to calculate the reference parameters.\n\nWhen calculating spatial parameters, you should calculate your own IMU based min_vel points instead of using the\nmocap derived ones.\nThese don't always align with real moments of no movement in the IMU data and hence might lead to issues with ZUPT\nbased algorithms.\n\nFor algorithms that rely on calculations on the entire signal (i.e. not just the strides within the mocap volume),\nkeep in mind, that the amount of additional movement in the data varies from trial to trial.\nSome trials just contain walking, others resting and walking, and some contain small jumps used as fallback\nsynchronization.\nHence, if you see unexpected results for specific trails, you might want to check the raw data.\n\n\n## Further Notes\nIn many cases clear drift in the Mocap data is observed.\nThe authors of the dataset corrected that drift before calculating the reference parameters using a linear drift\nmodel.\nFor further information see the two papers using the dataset [1]_ and [2]_.\n\n.. [1] Kanzler et al., \u201cInertial Sensor Based and Shoe Size Independent Gait Analysis Including Heel and Toe\n   Clearance Estimation.\u201d\n.. [2] Hannink et al., \u201cBenchmarking Foot Trajectory Estimation Methods for Mobile Gait Analysis.\u201d\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}